{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd1d704",
   "metadata": {},
   "source": [
    "# REDAS <=> Model (1)\n",
    "\n",
    "Built w/ Python 3.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87f75db",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Catalogue with Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e71be1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STORE YOUR WEBSITES HERE ###\n",
    "\n",
    "wiki = \"https://en.wikipedia.org/wiki/Special:Random\"\n",
    "gpt = \"https://www.assemblyai.com/blog/how-chatgpt-actually-works/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f157491",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> ENTER URL REFERENCE:\n",
    "    e.g. 'url = blockchain'   (or)   'url = random'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5555bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34771392",
   "metadata": {},
   "source": [
    "* <font color=\"blue\">Pass in the keywords from above.\n",
    "* E.g.: <font color=\"red\">url = gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac43df0",
   "metadata": {},
   "source": [
    "##### Credits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88ac39e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "program = \"Ready Script - Model (1)\"\n",
    "version = \"v.0.1-2a\"\n",
    "author = \"vera lo\"\n",
    "\n",
    "def credits():\n",
    "    print(\"\\n\" + program)\n",
    "    print(\"\\n\\nVersion: \" + version + \"\\nAuthor: \" + author)\n",
    "    !python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a1425b",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Libraries\n",
    "#### <font color=\"red\">(comment these out with an '#' if you already ran the script once,<br>or when you have already installed these libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9706a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### these here are necessary libraries ###\n",
    "# !pip install nltk\n",
    "# !pip install requests\n",
    "# !pip install beautifulsoup4\n",
    "# !pip install pyttsx3\n",
    "# !pip install pywhatkit\n",
    "# !pip install lxml\n",
    "\n",
    "### these are for (windows) ###\n",
    "# !python -m pip install pyaudio\n",
    "\n",
    "### these are for (linux) ###\n",
    "#!pip install PyAudio # if error lower version python <=3.6\n",
    "\n",
    "### additional (currently unsupported) libraries ###\n",
    "# !pip install SpeechRecognition\n",
    "# !pip install wikipedia\n",
    "# !pip install pyjokes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d82c3b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd8868e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user047\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import sys\n",
    "import csv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a88c1",
   "metadata": {},
   "source": [
    "### Scrape Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b501c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open URL\n",
    "scraped_data = urllib.request.urlopen(url)\n",
    "\n",
    "# Paragraphs\n",
    "article = scraped_data.read()\n",
    "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "# Get Title of Article\n",
    "response = requests.get(url=url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "# new_title = soup.find(id=\"firstHeading\")\n",
    "\n",
    "new_title =  soup.title.string\n",
    "\n",
    "# print(new_title)\n",
    "\n",
    "# Fox Article\n",
    "article_text = \"\"\n",
    "for p in paragraphs:\n",
    "    article_text += p.text\n",
    "\n",
    "# print(article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5f2fa7",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Cleaning Datata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36330999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Square Brackets and Extra Spaces\n",
    "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
    "# Any whitespace character \\s+\n",
    "article_text = re.sub(r'\\s+', ' ', article_text)\n",
    "# Removing special characters and digits\n",
    "formatted_article_text = re.sub('[^a-zA-ZÂ©]', ' ', article_text)\n",
    "# Any whitespace character \\s+\n",
    "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
    "# Clean the title\n",
    "new_title = re.sub(r'<[^>]*>', '', str(new_title))\n",
    "title = \"Your title is: \\\"\" + new_title + \"\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2361d5",
   "metadata": {},
   "source": [
    "### Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dd001aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert paragraphs to sentences\n",
    "sentence_list = nltk.sent_tokenize(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b4ba102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user047\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe3457a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62008163",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = {}\n",
    "for word in nltk.word_tokenize(formatted_article_text):\n",
    "    if word not in stopwords:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9420fea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a751c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = {}\n",
    "for sent in sentence_list:\n",
    "    for word in nltk.word_tokenize(sent.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1453ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0516d16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sentences = heapq.nlargest(3, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "summary = ' '.join(summary_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0343c300",
   "metadata": {},
   "source": [
    "### Adding the final ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05e924db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "import pywhatkit\n",
    "# import datetime\n",
    "# import wikipedia\n",
    "# import speech_recognition as sr\n",
    "# import pyjokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "648fdf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#listener = sr.Recognizer()\n",
    "engine = pyttsx3.init()\n",
    "voices = engine.getProperty('voices')\n",
    "engine.setProperty('voice', voices[1].id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b8cfda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "rate = engine.getProperty('rate')\n",
    "print(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2bed8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.setProperty('rate', 121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83f58294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk(text):\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "    \n",
    "def save(filen):\n",
    "    engine.save_to_file(formatted_article_text, filen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d01f15f",
   "metadata": {},
   "source": [
    "### <font color=\"red\">(SUMMARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c119a8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For example, a model designed to predict stock market prices might have an objective function that measures the accuracy of the model's predictions. A model's capability is typically evaluated by how well it is able to optimize its objective function, the mathematical expression that defines the goal of the model. A per-token KL penalty is added from the SFT model at each token to mitigate over optimization of the reward model.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621c94a7",
   "metadata": {},
   "source": [
    "##### Splash (Audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "504233b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ready = []\n",
    "ready.append(\"Your Ready Script, amazing.\")\n",
    "ready.append(\"Please help support this model, by visiting our website combine research dot com\")\n",
    "\n",
    "for i in ready:\n",
    "    talk(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e3adb4",
   "metadata": {},
   "source": [
    "### <font color=\"red\">(TITLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b41758ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your title is: \"How ChatGPT actually works\"\n"
     ]
    }
   ],
   "source": [
    "print(title)\n",
    "talk(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021f3f50",
   "metadata": {},
   "source": [
    "### <font color=\"red\">(Paragraphs Reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db2541c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep LearningSince its release the public has been playing with ChatGPT and seeing what it can do but how does ChatGPT actually work While the details of its inner workings have not been published we can piece together its functioning principles from recent research Developer Educator at AssemblyAI ChatGPT is the latest language model from OpenAI and represents a significant improvement over its predecessor GPT Similarly to many Large Language Models ChatGPT is capable of generating text in a wide range of styles and for different purposes but with remarkably greater precision detail and coherence It represents the next generation in OpenAI s line of Large Language Models and it is designed with a strong focus on interactive conversations The creators have used a combination of both Supervised Learning and Reinforcement Learning to fine tune ChatGPT but it is the Reinforcement Learning component specifically that makes ChatGPT unique The creators use a particular technique called Reinforcement Learning from Human Feedback RLHF which uses human feedback in the training loop to minimize harmful untruthful and or biased outputs We are going to examine GPT s limitations and how they stem from its training process before learning how RLHF works and understand how ChatGPT uses RLHF to overcome these issues We will conclude by looking at some of the limitations of this methodology In the context of machine learning the term capability refers to a model s ability to perform a specific task or set of tasks A model s capability is typically evaluated by how well it is able to optimize its objective function the mathematical expression that defines the goal of the model For example a model designed to predict stock market prices might have an objective function that measures the accuracy of the model s predictions If the model is able to accurately predict the movement of stock prices over time it would be considered to have a high level of capability for this task Alignment on the other hand is concerned with what we actually want the model to do versus what it is being trained to do It asks the question is that objective function consistent with our intentions and refers to the extent to which a model s goals and behavior align with human values and expectations For a simple concrete example say we train a bird classifier to classify birds as either sparrows or robins and we use log loss which measures the difference between the predicted probability distribution of the model and the true distribution as the training objective even though our ultimate goal is a high classification accuracy The model might have low log loss i e the model s capability is high but poor accuracy on the test set In fact the log loss is not perfectly correlated with accuracy in classification tasks This is an example of misalignment where the model is capable of optimizing the training objective but poorly aligned with our ultimate goal Large Language Models such as GPT are trained on vast amounts of text data from the internet and are capable of generating human like text but they may not always produce output that is consistent with human expectations or desirable values In fact their objective function is a probability distribution over word sequences or token sequences that allows them to predict what the next word is in a sequence more details on this below In practical applications however these models are intended to perform some form of valuable cognitive work and there is a clear divergence between the way these models are trained and the way we would like to use them Even though a machine calculated statistical distribution of word sequences might be mathematically speaking a very effective choice to model language we as humans generate language by choosing text sequences that are best for the given situation using our background knowledge and common sense to guide this process This can be a problem when language models are used in applications that require a high degree of trust or reliability such as dialogue systems or intelligent personal assistants While these powerful complex models trained on huge amounts of data have become extremely capable in the last few years when used in production systems to make human lives easier they often fall short of this potential The alignment problem in Large Language Models typically manifests as But where does this alignment problem stem from concretely Is it the very way language models are trained inherently prone to misalignment Next token prediction and masked language modeling are the core techniques used for training language models such as transformers In the first approach the model is given a sequence of words or tokens i e parts of words as input and is asked to predict the next word in the sequence For example if the model is given the input sentence The cat sat on the it might predict the next word as mat chair or floor because of the high probability of occurrence of these words given the previous context the language model is in fact able to estimate the likelihood of each possible word in its vocabulary given the previous sequence The masked language modeling approach is a variant of next token prediction in which some of the words in the input sentence are replaced with a special token such as MASK The model is then asked to predict the correct word that should be inserted in place of the mask For example if the model is given the sentence The MASK sat on the as input it might predict the next word as cat dog or rabbit One advantage of these objective functions is that it allows the model to learn the statistical structure of language such as common word sequences and patterns of word usage This generally helps the model generate more natural and fluent text and it is an essential step in the pre training phase of every language model However these objective functions can also lead to problems essentially because the model is not capable of distinguishing between an important error and an unimportant one To make a very simple example if the model is given the input sentence The Roman Empire MASK with the reign of Augustus it might predict began or ended as both words score high likelihood of occurrence indeed both sentences are historically correct even though the second choice implies a very different meaning More generally these training strategies can lead to a misalignment of the language model for some more complex tasks because a model which is only trained to predict the next word or a masked word in a text sequence may not necessarily be learning some higher level representations of its meaning As a result the model struggles to generalize to tasks or contexts that require a deeper understanding of language Researchers and developers are working on various approaches to address the alignment problem in Large Language Models ChatGPT is based on the original GPT model but has been further trained by using human feedback to guide the learning process with the specific goal of mitigating the model s misalignment issues The specific technique used called Reinforcement Learning from Human Feedback is based on previous academic research ChatGPT represents the first case of use of this technique for a model put into production But how exactly do the creators of ChatGPT use human feedback to attack the alignment problem The method overall consists of three distinct steps Step takes place only once while steps and can be iterated continuously more comparison data is collected on the current best policy model which is used to train a new reward model and then a new policy Let s now dive into the details of each step Note The rest of this article is based on the content of the InstructGPT paper According to OpenAI ChatGPT has been trained using the same methods as InstructGPT but with slight differences in the data collection setup source Unfortunately exact quantitative reports have yet to be made publicly available for ChatGPT The first step consists in collecting demonstration data in order to train a supervised policy model referred to as the SFT model Quite interestingly therefore in order to create a general purpose chatbot like ChatGPT the developers decided to fine tune on top of a code model rather than a pure text model Due to the limited amount of data for this step the SFT model obtained after this process is likely to output text which is still probabilistically not very user attentive and generally suffers from misalignment in the sense explained in the above sections The problem here is that the supervised learning step suffers from high scalability costs To overcome this problem instead of asking human labelers to create a much bigger curated dataset a slow and costly process the strategy is now to have the labelers rank different outputs of the SFT model to create a reward model let s explain this in more detail in the following section The goal is to learn an objective function the reward model directly from the data The purpose of this function is to give a score to the SFT model outputs proportional to how desirable these outputs are for humans In practice this will strongly reflect the specific preferences of the selected group of human labelers and the common guidelines which they agreed to follow In the end this process will extract from the data an automatic system that is supposed to mimic human preferences Here s how it works As for labelers it is much easier to rank the outputs than to produce them from scratch this process scales up much more efficiently In practice this dataset has been generated from a selection of k prompts and a variable number of the generated outputs for each prompt is presented to the each labeler during the ranking phase Reinforcement Learning is now applied to fine tune the SFT policy by letting it optimize the reward model The specific algorithm used is called Proximal Policy Optimization PPO and the fine tuned model is referred to as the PPO model What is PPO Here are the main takeaways of this method In this step the PPO model is initialized from the SFT model and the value function is initialized from the reward model The environment is a bandit environment which presents a random prompt and expects a response to the prompt Given the prompt and response it produces a reward determined by the reward model and the episode ends A per token KL penalty is added from the SFT model at each token to mitigate over optimization of the reward model Because the model is trained on human labelers input the core part of the evaluation is also based on human input i e it takes place by having labelers rate the quality of the model outputs To avoid overfitting to the judgment of the labelers involved in the training phase the test set uses prompts from held out OpenAI customers which are not represented in the training data The model is evaluated on three high level criteria The model is also evaluated for zero shot performance on traditional NLP tasks like question answering reading comprehension and summarization on some of which the developers observed performance regressions compared to GPT This is an example of an alignment tax where the RLHF based alignment procedure comes at the cost of lower performance on certain tasks The performance regressions on these datasets can be greatly reduced with a trick called pre train mix during training of the PPO model via gradient descent the gradient updates are computed by mixing the gradients of the SFT model and the PPO model A very clear limitation of the methodology as discussed in the InstructGPT paper on which ChatGPT is based according to its creators is the fact that in the process of aligning language models with human intentions the data for fine tuning the models is influenced by an intricate variety of subjective factors including In particular the authors point out the obvious fact that the labelers and researchers taking part in the training process may not be representative of all potential end users of the language model Apart from this clear intrinsic limitation we want to point out a few other possible shortcomings of the method problems not explicitly addressed as well as some open questions Lack of control study The reported results measure the performance of the final PPO model taking the SFT model as the baseline This can be misleading how can we know the improvements are actually due to the RLHF A proper yet expensive control study would consist in investing the exact same amount of labeler hours as those used to train the reward model into creating a larger curated SFT dataset with high quality demonstration data One would then be in the position to objectively measure the performance improvements of the RLHF methodology versus the supervised approach In very simple terms the lack of such a control study is leaving a fundamental question completely open is RLHF actually doing a good job in aligning language models Lack of ground truth for the comparison data labelers can often disagree on the ranking of the model s outputs Technically speaking the risk is to add a high potential variance to the comparison data without any ground truth Human preferences are just not homogeneous The RLHF method treats human preferences as if they were homogeneous and static Assuming that all people share the same values would be an obvious stretch at least on a large amount of topics of human knowledge Some recent research is starting to tackle this open problem differently Prompt stability testing for the reward model RM There seem to be no experiments investigating the sensitivity of the reward model in terms of changes in the input prompt If two prompts are syntactically different but are semantically equivalent can the RM show significant differences in the ranking of the model outputs In simpler terms how much does the quality of the prompt matter for the RM Wireheading type issues In RL approaches the model can sometimes learn to manipulate its own reward system to achieve a desired outcome leading to an over optimized policy This can push the model recreating some patterns that for some unknown reason make the reward model score high see Table in this paper from OpenAI for an explicit example of this behavior in language modeling ChatGPT puts a patch on this with the KL penalty term in the reward function Note that one is trying to optimize the RM input i e the PPO output in order to improve its output the reward score all the while constraining the input itself to be not too far away from some reference input the SFT output More details on the limitations of this approach in this recent preprint Enjoyed this article Follow our newsletter for more content like this Table of contents Developer Educator at AssemblyAI Software Engineer at AssemblyAI Head of Marketing at AssemblyAI API Support Engineer at AssemblyAI Â© AssemblyAI All rights reserved \n"
     ]
    }
   ],
   "source": [
    "print(formatted_article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dcd002",
   "metadata": {},
   "source": [
    "# <font color=\"purple\">Teaching (Audio)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3a354eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "talk(formatted_article_text)\n",
    "\n",
    "\n",
    "### (uncomment if need the unedited article for your reference) ###\n",
    "# print(article_text)\n",
    "# talk(article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5aef0e",
   "metadata": {},
   "source": [
    "\n",
    "### <font color=\"blue\">Credits Author:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07ee5eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ready Script - Model (1)\n",
      "\n",
      "\n",
      "Version: v.0.1-2a\n",
      "Author: vera lo\n",
      "Python 3.7.16\n"
     ]
    }
   ],
   "source": [
    "credits()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
