{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Python\n",
    "\n",
    "Natural language processing - commonly referred to as NLP - is the area of computer science dedicated to computers understanding human language such as speech or text. There are many examples of NLP applications including the following:\n",
    "\n",
    "- Sentiment Analysis - Determine the tone of text\n",
    "- Speech Recognition - Translate a sound clip to text\n",
    "- Predictive Text - Complete sentences based on a few words\n",
    "\n",
    "In this course, we will walk through the basics of NLP with Python libraries such as pandas, spaCy, and scikit-learn. We will cover the following topics:\n",
    "- Preprocessing\n",
    "- Token Frequency\n",
    "- Part of Speech Tagging\n",
    "- Named Entity Recognition\n",
    "- Text Similarity\n",
    "- Dependency Parsing\n",
    "\n",
    "This course assumes you have a beginner to intermediate knowledge of Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading PyCharm\n",
    "We will use PyCharm as the primary integrated development environment for this tutorial, but feel free to use your own IDE. To install Pycharm, select the Community Edition from [this link](https://www.jetbrains.com/pycharm/download/) (it's free!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy\n",
    "Throughout this tutorial, we will be using a spaCy, a popular open-source library for NLP in Python. The library is designed to help you create applications that process and understand text. spaCy offers several pre-made text processing pipelines on their site. The pipelines are packaged as [models](https://spacy.io/models/en) which can be downloaded. For this demo, we will download the small English model trained on text from blogs, news, and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "from pathlib import Path\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "In this course, we will analyze a sample of 500 Amazon Home and Kitchen product reviews. [The data](http://jmcauley.ucsd.edu/data/amazon/links.html) is provided by Julian McAuley at the University of California, San Diego and contains reviews from May 1996 - July 2014. In addition to reviews (ratings, text, helpfulness votes), McAuley provides product metadata (descriptions, category information, price, brand, and image features) and links (also viewed/also bought graphs). \n",
    "\n",
    "For this course, we will focus on the review data only. This data is a great example of the ways humans typically communicate through text and includes reviews with typos, run on sentences, and grammatical errors.\n",
    "\n",
    "McAuley provides the following functions to parse the JSON dataset and save it as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    f = open(path, 'rb')\n",
    "    for l in f:\n",
    "        yield eval(l)\n",
    "\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = getDF('./large_files/Home_and_Kitchen_5.json')\n",
    "df_sample = df.sample(n=500, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "Text preprocessing applies a variety of steps to text in order to clean or transform it for the computer to better understand. There are several common preprocessing steps. Let's take an example sentence and apply these steps to it.\n",
    "\n",
    "Sentence: \"She was offered the job 11 months ago.\"\n",
    "\n",
    "- Lowercase \n",
    "    - \"she was offered the job 11 months ago.\"\n",
    "- Remove punctuation \n",
    "    - \"she was offered the job 11 months ago\"\n",
    "- Remove numbers \n",
    "    - \"she was offered the job months ago\"\n",
    "- Remove stop words - remove words that are very common in the English language \n",
    "    - \"she offered job months ago\"\n",
    "- Tokenization - splitting the sentence up into tokens \n",
    "    - \"she\", \"offered\", \"job\", \"months\", \"ago\"\n",
    "- Stemming / lemmatization - transforming the token into its root form \n",
    "    - \"offer\", \"job\", \"month\", \"ago\"\n",
    "\n",
    "While these are all very popular preprocessing steps, they may not all be used on every project or even in this same order. The data you have and the problem you're trying to solve may add or remove any of these steps (and more) from your preprocessing. For example, if you want to see how many sentences are in the average Amazon review, you shouldn't remove punctuation.\n",
    "\n",
    "spaCy allows us to apply all of the preprocessing steps above in a single line of code by using [token attributes](https://spacy.io/api/token#attributes).\n",
    "- `token.lemma_` - lemmatizes the token\n",
    "- `token.is_alpha` - Removes punctuation and numbers (non-alphabetic characters)\n",
    "- `token.is_stop` - Removes stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out text preprocessing on sample text\n",
    "text = \"She was offered the job 11 months ago.\"\n",
    "doc = nlp(text)\n",
    "text_clean = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the entire dataframe\n",
    "def preprocess_text(spacy_doc: spacy.tokens.doc.Doc) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess a spacy Doc by lemmatizing, removing stop words, and removing non-alphabetical characters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    spacy_doc: spacy.tokens.doc.Doc\n",
    "        A spacy Doc object, i.e. a sequence of Token objects\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The cleaned text\n",
    "\n",
    "    \"\"\"\n",
    "    text_clean = [token.lemma_ for token in spacy_doc if token.is_alpha and not token.is_stop]\n",
    "    return ' '.join(text_clean)\n",
    "\n",
    "\n",
    "df_sample['spacy_doc'] = df_sample['reviewText'].apply(lambda x: nlp(x))\n",
    "df_sample['review_text_clean'] = df_sample['spacy_doc'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_sample['spacy_doc'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_sample['review_text_clean'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency\n",
    "With thousands of data points, we don't have time to read through each individual review to learn more about the data set. One way we can summarize the data is through the most popular words in reviews (term frequency). To do this, we use `CountVectorizer` from `scikit-learn`.\n",
    "\n",
    "CountVectorizer converts text into a matrix of token counts. It has a variety of [parameters](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) you can use to customize your results, but we will focus on `stop_words` and `ngram_range`.\n",
    "\n",
    "Although we previously removed stop words in our preprocessing steps, scikit-learn uses a different set of stop words than spaCy. Oftentimes it is beneficial to combine multiple stop words lists or create your own custom list to exclude common words that don't add value.\n",
    "\n",
    "N-grams are used to break text up into chunks. An example of a 1-gram is \"hello\", and an example of a 2-gram is \"hello there\". Modifying the `ngram_range` in `CountVectorizer` allows us to see the most popular words AND most popular phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(3, 3))\n",
    "docs = vectorizer.fit_transform(df_sample['review_text_clean'])\n",
    "features = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot the most popular words with `FreqDistVisualizer` from `scikit-yellowbrick`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = FreqDistVisualizer(features=features, size=(1080, 720))\n",
    "visualizer.fit(docs)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['token_count_all'] = df_sample['spacy_doc'].apply(lambda x: len(x))\n",
    "df_sample['token_count_clean'] = df_sample['review_text_clean'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['token_count_all'].value_counts().sort_index().plot.bar(figsize=(40,5), \n",
    "                                                                    title='All Tokens per Review',\n",
    "                                                                    xlabel='Tokens',\n",
    "                                                                    ylabel='Number of Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['token_count_clean'].value_counts().sort_index().plot.bar(figsize=(17,5), \n",
    "                                                                    title='Clean Tokens per Review',\n",
    "                                                                    xlabel='Tokens',\n",
    "                                                                    ylabel='Number of Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample[df_sample['token_count_clean'] == 271]['reviewText'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "Named Entity Recognition (NER) identifies real-world objects such as people, places, or things in text. NER is useful in many scenarios such as identifying and masking sensitive information such as names of people. spaCy recognizes [several different types of entities](https://v2.spacy.io/api/annotation#named-entities) and has a nice visualization to highlight all entities it recognized in text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df_sample['spacy_doc'][271416]\n",
    "print(doc)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(df_sample['spacy_doc'][271416], style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recognize all entities in all Amazon reviews\n",
    "df_entities = pd.DataFrame(columns=['index', 'spacy_doc', 'entity_text', 'entity_label', 'entity_start', 'entity_end'])\n",
    "\n",
    "index = 0\n",
    "for row in df_sample.itertuples():\n",
    "    for ent in row.spacy_doc.ents:\n",
    "        df_entities.at[index, 'index'] = row.Index\n",
    "        df_entities.at[index, 'spacy_doc'] = row.spacy_doc\n",
    "        df_entities.at[index, 'entity_text'] = ent.text\n",
    "        df_entities.at[index, 'entity_label'] = ent.label_\n",
    "        df_entities.at[index, 'entity_start'] = ent.start_char\n",
    "        df_entities.at[index, 'entity_end'] = ent.end_char\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See the most popular entities recognized\n",
    "df_entities['entity_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See what products are recognized\n",
    "df_filtered = df_entities[df_entities['entity_label'] == 'ORG'] # entity_label = ORG, PRODUCT, PERSON, WORK_OF_ART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['entity_text'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n",
    "Part-of-Speech tagging determines which [part of speech](https://en.wikipedia.org/wiki/Part_of_speech) each token is. This usually occurs behind the scenes before lemmatization since many words can serve as multiple parts of speech and may be lemmatized differently depending on the certain part of speech. Additionally, POS tagging is used as a foundation for NER and many other text processing steps. One real world application of POS tagging is to distinguish between words with the same spelling but different meanings for translation. For example, if a computer was translating \"Can you throw this can in the trash?\" to Spanish, it would need to know that \"can\" has two different parts of speech in this sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df_sample['spacy_doc'][271416]\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency Parsing\n",
    "\n",
    "displacy.render(df_sample['spacy_doc'][271416], style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df_sample['spacy_doc'][271416]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token.pos_ == 'ADJ' or token.pos_ == 'ADV':\n",
    "        print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_adverbs_adjectives(spacy_doc: spacy.tokens.doc.Doc) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of adjectives and adverbs in the text\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    spacy_doc: spacy.tokens.doc.Doc\n",
    "        A spacy Doc object, i.e. a sequence of Token objects\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The number of adverbs and adjectives in the text\n",
    "\n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    for token in spacy_doc:\n",
    "        if token.pos_ == 'ADJ' or token.pos_ == 'ADV':\n",
    "            counter+=1\n",
    "    \n",
    "    return counter   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['count_adj_adv'] = df_sample['spacy_doc'].apply(lambda x: count_adverbs_adjectives(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['count_adj_adv'].value_counts().sort_index().plot.bar(figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample[df_sample['count_adj_adv'] == 119]['reviewText'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
