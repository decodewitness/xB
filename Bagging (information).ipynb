{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df2e85c3",
   "metadata": {},
   "source": [
    "# BAGGING\n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "1. The decision trees can fit a complex pattern but they almost always overfit if the data is large, by overfit we mean a model that has \"high variance\" in it.\n",
    "\n",
    "2. One way to overcome this problem is \"model averaging\", the key tool to do the model averaging is called \"Bootstrap\" or \"resampling\".\n",
    "\n",
    "\n",
    "### Bootstrap\n",
    "\n",
    "Suppose we have 'N' data points and we want to estimate the mean.\n",
    "\n",
    "* Draw a sample with replacement 'B' times.\n",
    "    * each sample should be of size 'N'.\n",
    "* Calculate mean of each sample and collect it.\n",
    "* Estimated mean = average of mean of each sample.\n",
    "\n",
    "We could apply the same to our datasets too, but here our parameter of interest is 'accuracy' rather than a simple 'mean'.\n",
    "\n",
    "By combining models, we use the same dataset with the same model we come up with a generalized model that works better than a single model.\n",
    "\n",
    "This process is called 'Bootstrap aggregating' or in short 'Bagging'.\n",
    "\n",
    "\n",
    "### Bootstrap aggregating/Bagging\n",
    "\n",
    "We have a large dataset of 'N' rows and we want to aggregate 'B' different models.\n",
    "\n",
    "1. Draw a sample with replacement from this dataset each of size 'S'.\n",
    "    a. Wgere S<N\n",
    "                 \n",
    "2. Create a new model and fit the data to this model and collect the model.\n",
    "                 \n",
    "3. Iterate step-1 and step-2 for 'B' times.\n",
    "                 \n",
    "4. We predict using each model on the entire prediction data.\n",
    "    a. If regression: we take an average of all predicted values.\n",
    "    b. If classification: we do a voting mechanism between the models to choose the predicted class.\n",
    "\n",
    "                 \n",
    "### Research\n",
    "\n",
    "                 \n",
    "1. It is not strict that only 'Decision trees' should be used for bagging. We can use Logistic regression or SVM as well.\n",
    "                 \n",
    "2. High variance nature of Decision trees make them a preferable choice for Baggin than any other model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
