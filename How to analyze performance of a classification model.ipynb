{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e687bf5d",
   "metadata": {},
   "source": [
    "# Accuracy\n",
    "\n",
    "1. Model.score for classification models calculates accuracy of prediction.\n",
    "\n",
    "\n",
    "Y    P\n",
    "\n",
    "1    0    0\n",
    "1    1    1\n",
    "0    0    1\n",
    "1 == 1 -> 1 ---> 6/7 ---> 85.7\n",
    "1    1    1\n",
    "1    1    1\n",
    "0    0    1\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "\n",
    "* Helps us visualize model imbalances\n",
    "\n",
    "n=165   Predicted: No    Predicted: Yes\n",
    "\n",
    "Actual: TN = 50          FP = 10         60\n",
    "No\n",
    "\n",
    "Actual: FN = 5           TP = 100        105\n",
    "Yes\n",
    "\n",
    "        55                 110\n",
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "* True Positive (TP) is when we predict a data point to be \"yes\" and it's actually \"yes\".\n",
    "    a. we predict a person has a disease and they do have a disease.\n",
    "    \n",
    "* True Negative (TN) is when we predict a data point to be \"no\" and it's actually \"no\".\n",
    "    a. we predict a person doesn't have a disease and they don't have it as well.\n",
    "    \n",
    "* False Positive (FP) is when we predict a data point to be \"yes\" but it's actually \"no\", this is an error as we have predicted wrong, this type of error is also called \"Type 1 Error\".\n",
    "\n",
    "* False Negative is when we predict a data point to be \"no\" but it's actually \"yes\", this is an error as we have predicted wrong, this type of error is also called \"Type 2 Error\".\n",
    "    a. we predict a person doesn't have a disease but htey do have it.\n",
    "    \n",
    "* Accuracy = TP + TN / (TP + TN + FP + FN)\n",
    "\n",
    "* maximize the number of True positives and True negatives and minimize the number of false positives and false negatives.\n",
    "\n",
    "\n",
    "#### False Positive vs False Negatives\n",
    "\n",
    "1. Predicting a person doesn't have a disease when they do have it.\n",
    "    a. Leads to huge loss (False Negative)\n",
    "    \n",
    "    \n",
    "# Precision\n",
    "\n",
    "* Precision = True Positive / (True Positive + False Positive)\n",
    "\n",
    "1. how much percentage of our predicted \"yes\" are actually \"yes\"\n",
    "2. Precision is a good measure to determine when the costs of False Positive is high.\n",
    "    a. email spam detection - a false positive means that an email that is non-spam (actual negative) has been identified as spam (predicted spam).\n",
    "    b. The email user might lose important emails if the precision is not high for the spam detection model.\n",
    "    \n",
    "    \n",
    "# Recall\n",
    "\n",
    "* Recall = True Positives / (True Positives + False Negatives)\n",
    "\n",
    "1. how much percentage of total labeled \"yes\" are actually predicted \"yes\".\n",
    "2. Recall is a good measure to determine when the cost of False Negative is high.\n",
    "    a. Fraud detection and Disease prediction\n",
    "3. Recall is also called \"Sensitivity\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
