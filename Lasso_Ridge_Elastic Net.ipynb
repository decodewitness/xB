{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e75ab63",
   "metadata": {},
   "source": [
    "# Lasso, Ridge and Elastic Net\n",
    "\n",
    "\n",
    "* Ridge regularization - works well with penalizing the large weights.\n",
    "* Lasso regularization - removes unwanted features by penalizing their weight to zero.\n",
    "\n",
    "\n",
    "### Ridge Regularization\n",
    "\n",
    "SS(residuals) = Sum(n;i=l)(y(i)-y^(i))(2)\n",
    "\n",
    "Predictions = W.dot(X)\n",
    "\n",
    "W -> Weight vector\n",
    "X -> Feature vector\n",
    "\n",
    "Y^(model1) = W(model1).dot(X)\n",
    "Y^(model2) = W(model2).dot(X\n",
    "\n",
    "    1. Age = 18, Y_hat of an overfitted model is greater than the Y_hat of a perfect model.\n",
    "    \n",
    "    2. If Y_hat of model1 is greater than Y_hat of model2 then Weights of model1 should be greater than Weights of model2.\n",
    "    \n",
    "    3. Coefficients or weights of an overfitted model is greater than a good model. To address this issue we have \"Ridge regularization\" or \"L2 regularization\".\n",
    "    \n",
    "    4. When model weights are increasing, the (lambda)*weight^ also increases which increases the loss. Due to this effect model weights are shrinked and not overfitted.\n",
    "    \n",
    "    5. If (lambda) = 0, Ridge Regression = Linear Regression.\n",
    "    \n",
    "    \n",
    "### Lasso Regularization\n",
    "\n",
    "1. Higher degree polynomial functions can draw complex boundary lines.\n",
    "    a. similarly having more number of unwanted features will increase complexity thus overfitting the model.\n",
    "\n",
    "2. Lasso or \"L1\" regularization will address this issue.\n",
    "\n",
    "3. By setting the (lambda) large enough, Lasso eliminates the unwanted features by penalizing their weight very close to zero and sometimes zero as well.\n",
    "\n",
    "4. If (lambda) = 0, Lasso Regression = Linear regression\n",
    "\n",
    "\n",
    "### Elastic Net\n",
    "\n",
    "1. Uses both in combination of L1 and L2 regularization.\n",
    "\n",
    "2. L1 and L2 penalties have their own (lambda)'s, (lambda)2 for L2 and (lambda)1 for L1.\n",
    "\n",
    "3. To find out the best possible values of (lambda)'s we use a cross-validation or few searching techniques.\n",
    "\n",
    "    a. (lambda)1 = 0, Ridge regularization.\n",
    "    b. (lambda)2 = 0, Lasso regularization.\n",
    "    c. (lambda)1, (lambda)2 = 0, Linear regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
