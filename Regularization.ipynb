{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "315d497c",
   "metadata": {},
   "source": [
    "# REGULARIZATION\n",
    "\n",
    "\n",
    "### What is Overfitting?\n",
    "\n",
    "1. Pattern - As age of student increases their height increases.\n",
    "2. But there are 4 basketball players in the school and their height usually is higher than normal students.\n",
    "3. We expect our model to learn the signal more than the noise or randomness.\n",
    "    a. Signal - Pattern in Heights.\n",
    "    b. Noise/Randomness - Heights of the basketball players.\n",
    "\n",
    "\n",
    "### More about Overfitting\n",
    "\n",
    "Overfitting is a situation where our model has learnt the whole signal and noise and gave good predictions on training data but failed to perform well on the testing set.\n",
    "\n",
    "1. Model A has learnt the signal and left the noise - Ideal Model.\n",
    "2. Model B (overfit - will perform poorly) as it has high variance as it has learnt the whole variance (both signal and noise) from the training data.\n",
    "\n",
    "\n",
    "### Underfitting\n",
    "\n",
    "Underfitting is a situation where our model fails to learn both signal and noise and performs poorly on training and testing sets.\n",
    "\n",
    "1. Model A - Learnt the signal and left the noise - Ideal Model\n",
    "2. Model B - Irrespective of the signal or noise this model failed to learn the basic pattern in our data. (performs poorly)\n",
    "\n",
    "\n",
    "### Regularization\n",
    "\n",
    "1. It's very rare we get into underfitting problem, the major problem we often encounter is overfitting.\n",
    "2. Regularization minimizes the complexity of the model by penalizing the loss function to solve overfitting.\n",
    "\n",
    "\n",
    "# Applying Ridge, Lasso, Elastic Net\n",
    "\n",
    "\n",
    "\n",
    "### Features\n",
    "\n",
    "1. If features/column count = 1000, then we can assume that most of these features might not help in the prediction, and we want to eliminate most of these values, so use Lasso regularization.\n",
    "\n",
    "2. If we use Ridge in this case the weights might reduce but will never go to zero and due to high dimensionality we will still overfit.\n",
    "\n",
    "\n",
    "### High co-relations, Less Features\n",
    "\n",
    "1. If there are a lot of correlated features in our dataset, using Lasso will help remove the most correlated ones i.e., reduce the number of features and the complexity of the model as well.\n",
    "\n",
    "\n",
    "2. If our dataset has less number of features [n_features = 20] and the features have many less correlations then we should use Ridge regularization.\n",
    "\n",
    "    a. If we use Lasso in this case the weights of few features will become zero and this would lead to data loss eventually underfitting the model.\n",
    "\n",
    "\n",
    "3. You can use Elastic net everywhere.\n",
    "    \n",
    "    a. By tuning Lambda1 and Lambda2 we can get to that sweet spot of removing unwanted features as well and penalizing weights to a great extent for remaining features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
