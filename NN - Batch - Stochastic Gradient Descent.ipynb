{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c631b49",
   "metadata": {},
   "source": [
    "# NEURAL NETWORKS - Gradient Descent (information)\n",
    "\n",
    "* Gradient Descent also known as \"Back propagation\" is an optimization technique that is used to improve deep learning and neural network-based models by minimizing the loss function.\n",
    "\n",
    "* Training a Deep neural network\n",
    "\n",
    "* Gradient descent is an iterative approach which keeps updating weights to reach a point where loss is minimum.\n",
    "\n",
    "* The way we update the weights is by using the slope of loss to weights curve, we call this the gradient.\n",
    "\n",
    "1. Gradient calculation, which is nothing but slope.\n",
    "2. Weight updating.\n",
    "\n",
    "The variance of gradient descent revolves around addressing different ways of calculating these 2 steps.\n",
    "\n",
    "\n",
    "# Epoch\n",
    "\n",
    "We say that the model has performed an epoch if gradients for whole training data have been calculated and the weights have been updated.\n",
    "\n",
    "Training a model involves running multiple epochs.\n",
    "\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "    * Quantifies the amount of error in your predictions, it is a metric that is directly related to the performance of the model.\n",
    "    * If predictions are good, then the loss function would output a lower number and vice versa.\n",
    "    * Our Loss function should always justify 2 things:\n",
    "        * It should always be low.\n",
    "        * It should be differentiable.\n",
    "    * Weights and loss are related\n",
    "\n",
    "\n",
    "### Minimization\n",
    "    * As loss function is negative oriented the neural network always makes it less.\n",
    "\n",
    "\n",
    "# Diagram\n",
    "(parabolic curve):\n",
    "\n",
    "(loss)\n",
    "J(W)  .   .Initial weights\n",
    ".       . Global minimum\n",
    "....W = (weights)\n",
    "\n",
    "\n",
    "We calculate the slope of the curve differentiating loss by weights.\n",
    "    * d(y) by d(x).\n",
    "    \n",
    "\n",
    "# Gradient Descent Algorithms\n",
    "\n",
    "### Batch gradient descent algorithm.\n",
    "    \n",
    "    * We take the whole training data as a batch and pass it to the network. The network calculates the Gradient for the whole training data and updates the weights.\n",
    "    \n",
    "    * An epoch will update our weights once.\n",
    "    \n",
    "    ### Advantage\n",
    "    * Speed is very high.\n",
    "    \n",
    "    ### Disadvantage\n",
    "    * Poor learning in initial phases, high memory consumption\n",
    "    \n",
    "    * Use when the dataset is small and the problem statement is not too complex.\n",
    "\n",
    "# Stochastic gradient descent algorithm.\n",
    "\n",
    "* Here we take each data point and calculate its gradient.\n",
    "* Next we update the weights and repeat this process.\n",
    "\n",
    "* So if our dataset has 1000 data points, using stochastic gradient descent we will calculate gradients and update weight 1000 times in one epoch.\n",
    "\n",
    "* Use when the dataset is small and the problem statement is complex.\n",
    "\n",
    "    ### Advantage \n",
    "    \n",
    "    * Memory\n",
    "    \n",
    "    ### Disadvantages\n",
    "    \n",
    "    * Low Speed\n",
    "    * Overlearning (as the weights are updated very frequent, the model could overfit)\n",
    "    \n",
    "    \n",
    "# Mini Batch Gradient Descent\n",
    "\n",
    "* Here we internally batch the training data, we then calculate gradient and update weights for each batch.\n",
    "\n",
    "* We repeat this process until gradients over the whole training data are calculated and weights are updated.\n",
    "\n",
    "* So if our dataset has 1000 examples and our batch size is 100, then we update weights 10x per epoch.\n",
    "\n",
    "* Use if the dataset is large.\n",
    "\n",
    "    ### Advantage\n",
    "    \n",
    "    * Low memory consumption.\n",
    "    * High Speed\n",
    "    * Less chances of overfitting\n",
    "    \n",
    "    ### Disadvantages\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
