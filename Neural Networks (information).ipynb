{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a0b0c12",
   "metadata": {},
   "source": [
    "# Neural Networks (information)\n",
    "\n",
    "* A Neural Network is a series of algorithms that help us recognize relationships in a dataset through a process which mimics the way the human brain works.\n",
    "\n",
    "* It can adapt to changing input and generate the best results for us.\n",
    "\n",
    "* A \"neuron\" in a neural network is a mathematical function that collects and classifies information according to a specific architecture.\n",
    "\n",
    "\n",
    "### Architecture\n",
    "\n",
    "* 3 Main components in a neural network:\n",
    "    \n",
    "    * The input layer.\n",
    "        - The input is the beginning of the workflow, composed of artificial input neurons, and prepares the information for further processing by subsequent layers of neural networks.\n",
    "        \n",
    "    * The hidden layer.\n",
    "        - Applies weights to the inputs and direct them through activation functions for the output. Hidden layers perform nonlinear transformations of the inputs entered into the network.\n",
    "        \n",
    "    * The output layer.\n",
    "        - Produces output for the program.\n",
    "\n",
    "\n",
    "### Activation functions\n",
    "\n",
    "Activation functions are mathematical equations that determine the output of a neural network.\n",
    "\n",
    "The function is attached to each neuron in the network, and determines whether it should be activated or not, based on whether each neuron's input is relevant for the model's prediction.\n",
    "\n",
    "A transformation function that maps the input signals to output signals, for the functioning of the neural network.\n",
    "\n",
    "It helps us to determine the output of the program by normalizing the output values in a range of 0 to 1 or -1 to 1.\n",
    "\n",
    "\n",
    "### Different Activation Functions\n",
    "\n",
    "* Linear Activation function\n",
    "    * f(x) = x\n",
    "    * range from -infinity to infinity\n",
    "\n",
    "* Non Linear Activation function\n",
    "    (Slope):\n",
    "    * Derivative\n",
    "    * Differential\n",
    "\n",
    "* Monotonic function\n",
    "    * A function which is either entirely non-increasing or non-decreasing.\n",
    "\n",
    "* Sigmoid, tanh, relu, prelu, elu, leaky relu, etc.\n",
    "\n",
    "\n",
    "# When to use Sigmoid or Softmax\n",
    "\n",
    "* Binary classification problems (yes or no decision), we can use Sigmoid.\n",
    "\n",
    "* Multi-class classification problems (n-different classes), we can use the Softmax function.\n",
    "    * exponential of the power of the logit of a class divided by the sum of the exponentials of each class.\n",
    "\n",
    "* Magnitudes are also called Logits.\n",
    "    * Apple 0.46\n",
    "    * Banana 0.34\n",
    "    * Orange 0.20\n",
    "\n",
    "(sigma) -->(z)i = e(z)i / sum(K..j=i) e(z)j\n",
    "\n",
    "* e power z(i) in the enumerator is the exp. logit of i'th - class.\n",
    "* Denominator = sum of all exponentials of the logits of each class.\n",
    "* The sum of all outputs = 1.\n",
    "\n",
    "* We find the index with maximal probability and give that as the output.\n",
    "\n",
    "* Softmax(Apple) = e power 1.2 divided by (e power 1.2 + e power 0.9 + e power 0.4) = 0.46.\n",
    "\n",
    "* Only use Softmax when predicting more than 2 classes, else use Sigmoid.\n",
    "\n",
    "\n",
    "# Variety of Activation functions\n",
    "\n",
    "#### Sigmoid\n",
    "\n",
    "* A sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve.\n",
    "\n",
    "* The main reason for using the Sigmoid function is that the function exists between -1 and 1.\n",
    "\n",
    "* It is used for binary classification (yes or no).\n",
    "\n",
    "* It can also be used for the prediction of an outcome.\n",
    "\n",
    "#### Tanh\n",
    "\n",
    "Generally used for classification between 2 classes.\n",
    "\n",
    "* Tanh function is also known as Hyperbolic tangent Activation function.\n",
    "* Tanh is similar to the Sigmoid.\n",
    "* Range is between -1 and 1.\n",
    "* Looks like an \"S\"-curve too.\n",
    "* Tanh has a large area and better slope compared to Sigmoid. This helps models using Tanh activation to learn better.\n",
    "\n",
    "#### Relu\n",
    "\n",
    "R(z) = max(0, z)\n",
    "\n",
    "Relu is used for Convolutional Neural networks.\n",
    "\n",
    "* Is also known as Rectified Linear Unit Activation function.\n",
    "* Most used and accepted function.\n",
    "\n",
    "* It is half rectified from bottom, which means if you feed any negative data to Relu, it will return you a zero, and if you feed positive data it will return the exact number.\n",
    "\n",
    "* The range is from 0 to infinity.\n",
    "\n",
    "* The major issue is that all the negative values become 0, which decreases the ability of the model to fit or train from the data properly.\n",
    "\n",
    "* It will not map any negative values properly in a graph, because they are made 0.\n",
    "\n",
    "* Monotonic.\n",
    "\n",
    "#### Leaky Relu \n",
    "\n",
    "* Leaky Relu activation functions help us to increase the range of the ReLU function.\n",
    "\n",
    "* Usually A=(0.01) but you can change the value in different scenarios.\n",
    "    * If changed, it's called Randomized Relu.\n",
    "\n",
    "* Range from -infinity to infinity.\n",
    "\n",
    "* Monotonic.\n",
    "\n",
    "# Other Activation functions\n",
    "\n",
    "* Prelu\n",
    "* Elu\n",
    "\n",
    "\n",
    "# Features\n",
    "\n",
    "* Optimizers.\n",
    "* The Dropout Layer for neural networks.\n",
    "* Hyper Parameter Tuning.\n",
    "* Batch Normalization.\n",
    "\n",
    "\n",
    "### Different types of Neural Networks\n",
    "\n",
    "* Convolutional Neural Networks.\n",
    "* Recurrent Neural Networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
