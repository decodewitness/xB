{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f68662f9",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors\n",
    "\n",
    "\n",
    "### KNN Information\n",
    "\n",
    "* Makes highly accurate predictions.\n",
    "\n",
    "* Most effective algorithm to predict the missing values.\n",
    "\n",
    "* Apply whenever there is some relationship between the variables.\n",
    "\n",
    "* Use for applications that require higher accuracy, but do not require a human readable model.\n",
    "\n",
    "* The quality of the predictions depends on the distance measure, therefor the KNN algorithm is suitable for applications with sufficient domain knowledge available.\n",
    "\n",
    "* This knowledge supports the selection of an appropriate model.\n",
    "\n",
    "* K-NN algorithm can be used for imputing missing values of both categorical and continuous variables.\n",
    "\n",
    "* KNN algorithm is a type of lazy learning, where the computation for the generation of the predictions is differed until classification.\n",
    "\n",
    "* Although this method increases the cost of computation compared to other algorithms, KNN is still the better choice for applications where predictions are not requested frequently but accuracy is really important.\n",
    "\n",
    "\n",
    "### K-nearest Neighbors algorithm\n",
    "\n",
    "* The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems.\n",
    "\n",
    "* The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near each other.\n",
    "\n",
    "* Using 5 as a value of k in k-NN would minimize the leave one out cross validation accuracy.\n",
    "\n",
    "* When you decrease the k the variance will increase.\n",
    "\n",
    "\n",
    "### Two main properties\n",
    "\n",
    "* KNN is a lazy learning algorithm because it does not have a specialized training phase and uses all the data for training while classifying.\n",
    "\n",
    "* KNN is also a non-parametric learning algorithm because it doesn't assume anything about the underlying data.\n",
    "\n",
    "\n",
    "### KNN Algorithm\n",
    "\n",
    "* Uses feature similarity to understand the new data point, which means that the new data point will be assigned a value based on how closely it matches with the training set points.\n",
    "\n",
    "1. Load the data\n",
    "\n",
    "2. Initialize K to your chosen number of neighbors\n",
    "\n",
    "3. For each example in the data\n",
    "\n",
    "3.1 Calculate the distance between the query example and the current example from the data.\n",
    "\n",
    "3.2 Add the distance and index of the example to an ordered collection. \n",
    "\n",
    "4. Sort the ordered collection of distances and indices from smallest to largest (in ascending order) by the distances.\n",
    "\n",
    "5. Pick the first K entries from the sorted collection.\n",
    "\n",
    "6. Get the labels of the selected K entries.\n",
    "\n",
    "7. If regression, return the mean of the K labels.\n",
    "\n",
    "8. If classification, return the mode of the K labels.\n",
    "\n",
    "\n",
    "### Important to remember\n",
    "\n",
    "* We must be very careful while selecting the K value for our data.\n",
    "\n",
    "* In order to do that we run the KNN algorithm several times with different values of K and choose the K that reduces the number of errors we encounter while maintaining the algorithm's ability to accurately make predictions.\n",
    "\n",
    "\n",
    "### A few tips\n",
    "\n",
    "* As we decrease the value of K to 1, our predictions become less stable.\n",
    "\n",
    "* Inversely, as we increase the value of K, our predictions become more stable due to majority voting / averaging, and thus, more likely to make more accurate predictions.\n",
    "\n",
    "* In cases where we are taking a majority vote, among labels, we usually make K an odd number to have a tiebreaker.\n",
    "\n",
    "\n",
    "### Advantages\n",
    "\n",
    "* It is a very simple algorithm to understand and interpret.\n",
    "\n",
    "* It is very useful for nonlinear data because there is no assumption about data in this algorithm.\n",
    "\n",
    "* It is a versatile algorithm as we can use it for classification as well as regression.\n",
    "\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "* It is computationally a bit expensive because it stores all the training data.\n",
    "\n",
    "* High memory storage required as compared to other supervised learning algorithms.\n",
    "\n",
    "* It is very sensitive to the scale of data as well as irrelevant features.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
