{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f3f74a8",
   "metadata": {},
   "source": [
    "# Naive Bayes algorithm\n",
    "\n",
    "\n",
    "### Information\n",
    "\n",
    "* Works very efficiently when the data set is comparatively small and the dimension is relatively small.\n",
    "\n",
    "* Won't work best with a dataset having larger dimensions or larger features.\n",
    "\n",
    "* Naive Bayes treat all the variables equally important, why we use Gaussian Naive Bayes instead in most real life scenarios.\n",
    "\n",
    "* We generally use the gaussian naive bayes algorithm for better results because gaussian distribution assigns weights to the attributes depending on the distribution.\n",
    "\n",
    "* A Naive Bayes classifier is represented with (2n + 1).\n",
    "\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "* Predictive Classification modelling.\n",
    "\n",
    "* Naive Bayes is used to predict the outcome of independent events.\n",
    "\n",
    "* Naive Bayes comes from Sklearn.\n",
    "\n",
    "* Let's imagine this, an electric supplier would like to supply specific units of electric current to various factory divisions based on their past trends of power consumption.\n",
    "\n",
    "* To simplife the process, they plan to categorize the factory divisions into three groups-low, medium, and high-power consumers-based on the knowledge of how much electricity to supply.\n",
    "\n",
    "* Naive Bayes is a supervised machine-learning algorithm that uses the Bayes' Theorem, which assumes that features are statistically independent.\n",
    "\n",
    "* The theorem relies on the naive assumption that input variables are independent of each other, i.e. there is no way to know anything about other variables when given an additional variable.\n",
    "\n",
    "\n",
    "### Bayes Theorem\n",
    "\n",
    "P(A|B) = P(B|A)P(A) / P(B)\n",
    "\n",
    "P(A|B) = The probability of event A (hypothesis) occurring given that B (evidence) has occurred. This is called the posterior probability.\n",
    "\n",
    "P(B|A) = The probability of the event B (evidence) occurring given that A (hypothesis) has occurred.\n",
    "\n",
    "P(A) = The probability of event A (hypothesis) occurring.\n",
    "P(B) = The probability of event B (evidence) occurring.\n",
    "\n",
    "\n",
    "### Maximum A Posteriori (MAP) hypothesis\n",
    "\n",
    "* The posterior probability for a number of different hypotheses, we can select the hypothesis with the highest probability.\n",
    "\n",
    "* This is the maximum probable hypothesis and may formally be called the maximum a posteriori (MAP) hypothesis.\n",
    "\n",
    "MAP(A) = max(P(A|N))\n",
    "\n",
    "\n",
    "### Why naive Bayes is called naive Bayes?\n",
    "\n",
    "* It is called naive Bayes or idiot Bayes because the calculation of the probabilities for each hypothesis is simplified to make their calculation tractable.\n",
    "\n",
    "* Rather than attempting to calculate the values of each attribute value P(B1, 2, B3|A), they are assumed to be conditionally independent given the target value and calculated as P(B1|A) * P(B2|A) and so on.\n",
    "\n",
    "* This is a very strong assumption that is most unlikely in real data. The attributes do not interact.\n",
    "\n",
    "* The approach performs surprisingly well on data where this assumption does not hold.\n",
    "\n",
    "\n",
    "### Naive Bayes models use Probabilities\n",
    "\n",
    "* A list of probabilities is stored to file for a learned naive Bayes model. They are namely Class and Conditional probabilities.\n",
    "\n",
    "* We infer that Training is fast in Naive Bayes as the probability of each class and the probability of each class given different input (x) values need to be calculated.\n",
    "\n",
    "* No coefficients need to be fitted by optimization procedures.\n",
    "\n",
    "\n",
    "### EXAMPLE:\n",
    "\n",
    "Sample Dataset\n",
    "    OUTLOOK  TEMP.  HUMID. WINDY   PLAY GOLF\n",
    "0   Rainy     Hot   High   False   No\n",
    "1   Rainy     Hot   High   True    No\n",
    "2   Overcast  Hot   High   False   No\n",
    "3   Sunny     Mild  High   False   Yes\n",
    "\n",
    "\n",
    "### Let's understand the problem\n",
    "\n",
    "We classify whether the day is suitable for playing golf.\n",
    "\n",
    "* If we take the first row we see it's not suitable for playing golf.\n",
    "* We make 2 assumptions here:\n",
    "    * If the temp is hot, it does not necessary mean that the humidity is high.\n",
    "    * All the predictors have an equal effect on the outcome.\n",
    "    \n",
    "### Insights\n",
    "\n",
    "* assigning y as the class variable (play golf), which represents if it is suitable to play golf or not given the conditions.\n",
    "\n",
    "* X as x_1, x_2....x_n representing the featurwes, i.e. they can be mapped to outlook, temperature, humidity and windy.\n",
    "\n",
    "* Obtain the values for each by looking at the dataset and substitute them into the equation.\n",
    "\n",
    "* For all entries in the dataset the denominator does not change, it remains static.\n",
    "\n",
    "* There for the denominator can be removed and proportionality can be introduced.\n",
    "\n",
    "* The class variable y has only two outcomes, yes or no.\n",
    "\n",
    "* There could be cases where the classification can be multi-variant, there for we need to find the class Y with maximum probability.\n",
    "\n",
    "\n",
    "### Gaussian Naive Bayes\n",
    "\n",
    "* When the predictors take up a continuous value and are not discrete, we assume the values are sample from a gaussian distribution.\n",
    "\n",
    "* When the variables in the dataset change the formula for conditional probability changes too.\n",
    "\n",
    "\n",
    "### Pros and Cons of Naive Bayes\n",
    "\n",
    "##### Pros\n",
    "\n",
    "* Very fast compared to complicated algorithms.\n",
    "* It works well with high-dimensional data such as text classification, email spam detection.\n",
    "\n",
    "##### Cons\n",
    "\n",
    "* Less accurate than complicated algorithms. Speed comes at a cost!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
