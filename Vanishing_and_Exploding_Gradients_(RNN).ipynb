{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vanishing and Exploding Gradients (RNN).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Vanishing and Exploding Gradients\n",
        "\n",
        "## Backpropagation in RNN\n",
        "\n",
        "### 1. Forward pass in RNN moves forward across time while updating the hidden state based on previous hidden and current input.\n",
        "\n",
        "### 2. It generates output at every step, ie., it calculates the loss at each timestep.\n",
        "\n",
        "### 3. All individual losses are aggregated to shape an total loss of the network.\n",
        "\n",
        "\n",
        "The weight updates using the overall loss through each individual timestep and even across timesteps.\n",
        "\n",
        "### 4. Errors flow back in time from our current timestep x(t) to the first timestep x(0).\n",
        "\n",
        "\n",
        "## Issues with BPTT\n",
        "\n",
        "### 1. When performing backpropagation through time network performs a lot of matrix operations involving weight matrix W(hh).\n",
        "\n",
        "### 2. Computing gradients in 1st timestep h(0) involves many factors of weight matrix W(hh) and repeated gradient computations of h(t) to h(1).\n",
        "\n",
        "This can lead to Two problems:\n",
        "\n",
        "### 1. Exploding gradients.\n",
        "### 2. Vanishing gradients.\n",
        "\n",
        "# Exploding Gradients\n",
        "\n",
        "### 1. If gradient value > 1 in series of multiplications, then there is a chance for gradients to explode while backpropagation.\n",
        "\n",
        "##Example:\n",
        "  ### 1. 1power 100 = 13780.61, while 1power 100 = 1.\n",
        "##Solution:\n",
        "  ### 1. Gradient Clipping\n",
        "    ### a. we scale down large values to less than 1.\n",
        "\n",
        "# Vanishing Gradients\n",
        "\n",
        "### 1. If gradients value < 1 in series of multiplications, then there is a chance for gradients to vanish while backpropagation.\n",
        "\n",
        "##Example\n",
        "  ### 1. 0.9 power 100=0.0000265 while 1power 100 = 1\n",
        "##Solution\n",
        "  ### 1. Choosing good activation functions.\n",
        "  ### 2. Better weight initializations.\n",
        "  ### 3. Changing network architecture.\n",
        "\n",
        "\n",
        "# Choosing Good Activation Function\n",
        "\n",
        "### 1. ReLu and its variants are good when it comes to gradient flow.\n",
        "  ###a. Using ReLU prevents F(1) from shrinking the gradients when x > 0.\n",
        "\n",
        "\n",
        "# Weight Initialization\n",
        "\n",
        "### Parameter initialization\n",
        "\n",
        "##### Initialize weights to identity matrix\n",
        "##### Initialize biases to zero\n",
        "\n",
        "\n",
        "####         1  0  0  ...   0\n",
        "####         0  1  0  ...   0\n",
        "####         0  0  1  ...   0\n",
        "#### I(n) = {.  .  .  .     .\n",
        "####         .  .  .    .   . \n",
        "####         .  .  .      . .\n",
        "####         0  0  0  ...   1\n",
        "\n",
        "### This helps prevents the weight from shrinking to zero.\n",
        "\n",
        "\n",
        "# Change Network Architecture."
      ],
      "metadata": {
        "id": "LOvMu9twP3_M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7SRvqGaPjqi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}