{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP07NUnXLC8idsYL3NjVEqU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Importing the required libraries"],"metadata":{"id":"bxZO3p2VMdfz"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"IuhNUN0_MVzl","executionInfo":{"status":"ok","timestamp":1651369804082,"user_tz":-120,"elapsed":1406,"user":{"displayName":"starter kit","userId":"18012619874301147270"}}},"outputs":[],"source":["import numpy as np\n","import nltk\n","import string\n","import random"]},{"cell_type":"markdown","source":["Importing and reading the corpus"],"metadata":{"id":"dI8wCfxTMhYh"}},{"cell_type":"code","source":["f=open('corpus.txt','r',errors = 'ignore')\n","raw_doc=f.read()\n","raw_doc=raw_doc.lower() # converts text to lowercase\n","nltk.download('punkt') # using Punkt tokenizer\n","nltk.download('wordnet') # using the WordNet dictionary\n","sent_tokens = nltk.sent_tokenize(raw_doc) # converst doc to list of sentences\n","word_tokens = nltk.word_tokenize(raw_doc) # converts doc to list of words"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q9TOl6rhMjxP","executionInfo":{"status":"ok","timestamp":1651369806972,"user_tz":-120,"elapsed":539,"user":{"displayName":"starter kit","userId":"18012619874301147270"}},"outputId":"0ce1b04b-7444-43fa-baf3-dae942741b08"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["Example of sentence tokens"],"metadata":{"id":"nSFHVxfnNDRS"}},{"cell_type":"code","source":["sent_tokens[:2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AQ9sAV3LNF8p","executionInfo":{"status":"ok","timestamp":1651369809988,"user_tz":-120,"elapsed":212,"user":{"displayName":"starter kit","userId":"18012619874301147270"}},"outputId":"32465587-f823-4088-eb9b-1ef53e5abdf8"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data,[1][2] and apply knowledge and actionable insights from data across a broad range of application domains.',\n"," 'data science is related to data mining, machine learning and big data.']"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["Example of word tokens"],"metadata":{"id":"AnKLz-CRN4Oq"}},{"cell_type":"code","source":["word_tokens[:2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mlbN5LxPOCae","executionInfo":{"status":"ok","timestamp":1651369812512,"user_tz":-120,"elapsed":222,"user":{"displayName":"starter kit","userId":"18012619874301147270"}},"outputId":"552e7315-2d30-431b-cd8f-c6280ae5aa40"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['data', 'science']"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["Text preprocessing"],"metadata":{"id":"93ZwMEDNOGB8"}},{"cell_type":"code","source":["lemmer = nltk.stem.WordNetLemmatizer()\n","# Wordnet is a semantically oriented dictionary of English included in NLTK\n","def LemTokens(tokens):\n","  return [lemmer.lemmatize(token) for token in tokens]\n","remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n","def LemNormalize(text):\n","  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"],"metadata":{"id":"Treg-Pl3OISx","executionInfo":{"status":"ok","timestamp":1651369814941,"user_tz":-120,"elapsed":203,"user":{"displayName":"starter kit","userId":"18012619874301147270"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Defining the greeting function"],"metadata":{"id":"IjDWU7CZO2M1"}},{"cell_type":"code","source":["GREET_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\")\n","GREET_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad you are talking to me\"]\n","def greet(sentence):\n","  for word in sentence.split():\n","    if word.lower() in GREET_INPUTS:\n","      return random.choice(GREET_RESPONSES)"],"metadata":{"id":"x7sTqfpTO5AK","executionInfo":{"status":"ok","timestamp":1651369820062,"user_tz":-120,"elapsed":211,"user":{"displayName":"starter kit","userId":"18012619874301147270"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Response generation"],"metadata":{"id":"k-JO5E7gQHLs"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity"],"metadata":{"id":"UQ96StQjQJ1u","executionInfo":{"status":"ok","timestamp":1651369822108,"user_tz":-120,"elapsed":203,"user":{"displayName":"starter kit","userId":"18012619874301147270"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def response(user_response):\n","  robo1_response=''\n","  TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n","  tfidf = TfidfVec.fit_transform(sent_tokens)\n","  vals = cosine_similarity(tfidf[-1], tfidf)\n","  idx = vals.argsort()[0][-2]\n","  flat = vals.flatten()\n","  flat.sort()\n","  req_tfidf = flat[-2]\n","  \n","  if(req_tfidf==0):\n","    robo1_response=robo1_response+\"I am sorry! I do not understand you.\"\n","    return robo1_response\n","  else:\n","    robo1_response = robo1_response+sent_tokens[idx]\n","    return robo1_response"],"metadata":{"id":"rYCBqEOcQT_r","executionInfo":{"status":"ok","timestamp":1651369825093,"user_tz":-120,"elapsed":221,"user":{"displayName":"starter kit","userId":"18012619874301147270"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Defining conversation start/end protocols"],"metadata":{"id":"Pftl_Xo9Rqbq"}},{"cell_type":"code","source":["flag = True\n","print(\"BOT: My name is Ninja. Let's have a conversation! Also, if you want to exit at any time, just type 'Bye'!\")\n","while(flag==True):\n","  user_response = input()\n","  user_response=user_response.lower()\n","  if(user_response!='bye'):\n","    if(user_response=='thanks' or user_response=='thank you'):\n","      flag=False\n","      print(\"BOT: You are welcome...\")\n","    else:\n","      if(greet(user_response)!=None):\n","        print(\"BOT: \"+greet(user_response))\n","      else:\n","        sent_tokens.append(user_response)\n","        word_tokens=word_tokens+nltk.word_tokenize(user_response)\n","        final_words=list(set(word_tokens))\n","        print(\"BOT: \", end=\"\")\n","        print(response(user_response))\n","        sent_tokens.remove(user_response)\n","  else:\n","    flag=False\n","    print(\"BOT: Goodbye! Take care <3.\")"],"metadata":{"id":"KPK6czHBR31n"},"execution_count":null,"outputs":[]}]}