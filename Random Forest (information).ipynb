{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c07b1e94",
   "metadata": {},
   "source": [
    "# Random Forest (information)\n",
    "\n",
    "\n",
    "1. Average the results from high variance and low bias models.\n",
    "2. Trees are more correlated.\n",
    "\n",
    "Models can be quite the same in which case we train 2 models which is a waste.\n",
    "Why we use Random Forest.\n",
    "\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "1. In the same way we select random samples, we can also select random features which we want to train on.\n",
    "    a. This will help trees to be more de-correlated because we cannot estimate the initial split of a decision tree.\n",
    "    \n",
    "2. How to choose those random sets of features. Number of Features.\n",
    "    a. d = floor(sqrt(D)), for classification problem.\n",
    "    b. d = floor(D/3), for regression problem\n",
    "    \n",
    "Here 'D' is the total number of features and 'd' is the random set of features we selected.\n",
    "\n",
    "3. If D<15, max_features = 0.5, else if D>40 -> Inventors recommendation\n",
    "\n",
    "\n",
    "### Pseudo Algorithm\n",
    "\n",
    "Suppose we have a dataset of 'N' samples and 'D' features.\n",
    "\n",
    "1. Draw a sample with replacement each of size 'S'.\n",
    "    a. number of random features 'd', where d=sqrt(D)\n",
    "\n",
    "2. Create a new model and fit the model to the current sample.\n",
    "\n",
    "3. Repeat step-1, 2 -> B times and keep collecting all trained models.\n",
    "\n",
    "4. Aggregate all the models while doing prediction (same as bagging).\n",
    "\n",
    "\n",
    "### Understanding the Parameters of Random Forest\n",
    "\n",
    "Google: randomforrestclassifier\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "We need to handle tree depth with min_samples_split (a sweet spot is: 5 or 7) or use min_samples_leaf. A high number here can underfit the model and a low number can overfit.\n",
    "\n",
    "Same rules apply for min_samples_leaf.\n",
    "\n",
    "max_features is for random selection of columns.\n",
    "If I want to randomly select 50% of features we select 0.5 for this parameter.\n",
    "\n",
    "oob_score is one of the main parameters.\n",
    "As we only train a tree in random forest with a subset of rows and columns, we can use the remaining set of rows for testing of that tree for better utilization of data, that's what oob does for us.\n",
    "\n",
    "This parameter becomes handy when we have very little data to train out model.\n",
    "In that case we can train our model on the whole data and consider oob_score as the validation score.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
